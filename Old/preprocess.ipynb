{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "#import deepgraph as dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "https://www.synapse.org/#!Synapse:syn2812961 \n",
    "\n",
    "https://www.kaggle.com/murats/gene-expression-cancer-rnaseq?fbclid=IwAR3Mj1xpvD_sBBwn6ivsPf04pvWIm7n3QCKwSdb_bmxhFL2iEXvCGIParIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'unc.edu_LUSC_IlluminaHiSeq_RNASeqV2.geneExp.tsv' does not exist: b'unc.edu_LUSC_IlluminaHiSeq_RNASeqV2.geneExp.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-add3e2c3eef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mLGG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unc.edu_LGG_IlluminaHiSeq_RNASeqV2.geneExp.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLUCS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unc.edu_LUSC_IlluminaHiSeq_RNASeqV2.geneExp.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mPRAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unc.edu_PRAD_IlluminaHiSeq_RNASeqV2.geneExp.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'unc.edu_LUSC_IlluminaHiSeq_RNASeqV2.geneExp.tsv' does not exist: b'unc.edu_LUSC_IlluminaHiSeq_RNASeqV2.geneExp.tsv'"
     ]
    }
   ],
   "source": [
    "LGG = pd.read_csv(\"unc.edu_LGG_IlluminaHiSeq_RNASeqV2.geneExp.tsv\", sep='\\t')\n",
    "LUCS = pd.read_csv(\"unc.edu_LUSC_IlluminaHiSeq_RNASeqV2.geneExp.tsv\", sep='\\t')\n",
    "PRAD = pd.read_csv(\"unc.edu_PRAD_IlluminaHiSeq_RNASeqV2.geneExp.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows: Genes \\\n",
    "Columns: Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGG.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGG.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to build a coexpression matrix?\n",
    "\n",
    "https://link.springer.com/protocol/10.1007%2F978-1-4939-7747-5_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation\n",
    "def calculate_pearson(data, length):\n",
    "    '''\n",
    "    Input: Data sequence\n",
    "    Output: Person matrix\n",
    "    '''\n",
    "    gene_ids = data['gene_id'][:length]\n",
    "    n = len(gene_ids)\n",
    "    pmatrix = np.empty((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            gene1 = list(data.iloc[i, :].values)[1:][:length]\n",
    "            gene2 = list(data.iloc[j, :].values)[1:][:length]\n",
    "            # note: if either one of gene is all zeros, r and p are nan\n",
    "            r, p= pearsonr(gene1, gene2)\n",
    "            pmatrix[i][j] = r\n",
    "    return pmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-processing for pearson r computation\n",
    "# https://deepgraph.readthedocs.io/en/latest/tutorials/pairwise_correlations.html?fbclid=IwAR019mCREkqAlWLD_tDtVn_3LLj-NkAIyJJZMpr2_nYh_sEtDV404Eac5Xc\n",
    "\n",
    "# tranpose to line up with demonstration in the link above\n",
    "LGG_T = np.array(LGG.transpose().values)\n",
    "\n",
    "# compute ranked variables for Spearman's correlation coefficients\n",
    "LGG_T = LGG_T.argsort(axis=1).argsort(axis=1)\n",
    "# whiten variables for fast parallel computation later on\n",
    "LGG_T = (LGG_T - LGG_T.mean(axis=1, keepdims=True))/ LGG_T.std(axis=1, keepdims=True)\n",
    "\n",
    "# save in binary format\n",
    "np.save(\"LGG_T\", LGG_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the given to_pos parameter is too large, 168634 > g.n*(g.n-1)/2=142845.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/jinglin/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"<ipython-input-4-2ce0759b4d7d>\", line 33, in create_ei\n    from_pos = from_pos, to_pos=to_pos)\n  File \"/Users/jinglin/miniconda3/lib/python3.7/site-packages/deepgraph/deepgraph.py\", line 612, in create_edges\n    transfer_features, verboseprint, logfile, hdf_key)\n  File \"/Users/jinglin/miniconda3/lib/python3.7/site-packages/deepgraph/deepgraph.py\", line 4823, in _matrix_iterator\n    '{} > g.n*(g.n-1)/2={}'.format(to_pos, N*(N-1)/2))\nAssertionError: the given to_pos parameter is too large, 168634 > g.n*(g.n-1)/2=142845.0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2ce0759b4d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_processes\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_ei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mLGG_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: the given to_pos parameter is too large, 168634 > g.n*(g.n-1)/2=142845.0"
     ]
    }
   ],
   "source": [
    "# parameters (change these to control RAM usage)\n",
    "step_size = 1e5\n",
    "n_processes = 1e4\n",
    "n_features, n_samples = LGG.shape\n",
    "\n",
    "# load data as memory-map\n",
    "LGG_T = np.load(\"LGG_T.npy\", mmap_mode=\"r\")\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({\"index\":range(LGG_T.shape[0])})\n",
    "\n",
    "# connector function to compute pairwise pearson correlations\n",
    "def corr(index_s, index_t):\n",
    "    features_s = LGG_T[index_s]\n",
    "    features_t = LGG_T[index_t]\n",
    "    corr = np.einsum(\"ij,ij->i\", features_s, features_t) / n_samples\n",
    "    return corr\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# parallel computation\n",
    "def create_ei(i):\n",
    "        \n",
    "    from_pos = pos_array[i]\n",
    "    to_pos = pos_array[i+1]\n",
    "    \n",
    "    # initiate DeepGraph\n",
    "    g = dg.DeepGraph(v)\n",
    "    \n",
    "    # crate edges\n",
    "    g.create_edges(connectors=corr, step_size=step_size,\n",
    "                  from_pos = from_pos, to_pos=to_pos)\n",
    "    \n",
    "    # store edge table\n",
    "    g.e.to_pickle(\"tmp/correlations/{}.pickle\".format(str(i).zfill(3)))\n",
    "    \n",
    "# computation\n",
    "#os.makedirs(\"tmp/correlations\", exist_ok=True)\n",
    "indices = np.arange(0, n_processes-1, dtype=int)\n",
    "p = Pool()\n",
    "for _ in p.imap_unordered(create_ei, indices):\n",
    "    pass\n",
    "LGG_T.close()\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store correlation values\n",
    "files = os.listdir('tmp/correlations/')\n",
    "files.sort()\n",
    "store = pd.HDFStore('e.h5', mode='w')\n",
    "for f in files:\n",
    "    et = pd.read_pickle('tmp/correlations/{}'.format(f))\n",
    "    store.append('e', et, format='t', data_columns=True, index=False)\n",
    "store.close()\n",
    "\n",
    "# load correlation table\n",
    "e = pd.read_hdf('e.h5')\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LUCS_T = np.array(LUCS.transpose().values)\n",
    "PRAD_T = np.array(PRAD.transpose().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg_num_genes, lgg_num_patients = LGG.shape\n",
    "LGG_matrix = calculate_pearson(LGG, lgg_num_genes)\n",
    "lucs_num_genes, lucs_num_patients = LUCS.shape\n",
    "LUCS_matrix = calculate_pearson(LUCS, lucs_num_genes)\n",
    "prad_num_genes, prad_num_patients = PRAD.shape\n",
    "PRAD_matrix = calculate_pearson(PRAD, prad_num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGG_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, data):\n",
    "    outfile = open(filename, 'wb')\n",
    "    pickle.dump(data, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def load_pkl(filename):\n",
    "    file = open(filename, 'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "    return object_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(\"lgg_pearson_matrix.p\", LGG_matrix)\n",
    "write_to_file(\"lucs_pearson_matrix.p\", LUCS_matrix)\n",
    "write_to_file(\"prad_pearson_matrix.p\", PRAD_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for symmetric\n",
    "def is_symmetric(matrix):\n",
    "    n = len(matrix)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if matrix[i][j] != matrix[j][i]:\n",
    "                # can be nan\n",
    "                if not np.isnan(matrix[i][j]) or not np.isnan(matrix[i][j]):\n",
    "                    return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_symmetric(LGG_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, adj_matrix, actual_name):\n",
    "    # undirected, weighted graph\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(adj_matrix)):\n",
    "        if actual_name:\n",
    "            G.add_node(data[\"gene_id\"].iloc[i])\n",
    "        else:\n",
    "            G.add_node(i)\n",
    "        for j in range(i+1, len(adj_matrix)):\n",
    "            if not np.isnan(adj_matrix[i][j]): \n",
    "                if actual_name:\n",
    "                    G.add_edge(data[\"gene_id\"].iloc[i], data[\"gene_id\"].iloc[j], weight=adj_matrix[i][j])\n",
    "                else:\n",
    "                    G.add_edge(i, j, weigt=adj_matrix[i][j])\n",
    "    return G\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_graph(LGG, LGG_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to check graph is correct\n",
    "print (\"Number of nodes: \", nx.number_of_nodes(G) == len(LGG_matrix))\n",
    "print (\"Number of selfloop: \", nx.number_of_selfloops(G))\n",
    "print (G.get_edge_data(0,1), LGG_matrix[0][1])\n",
    "print (G.get_edge_data(2, 4), LGG_matrix[2][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"nodes number: {}, edges number: {}\".format(nx.number_of_nodes(G), nx.number_of_edges(G)))\n",
    "print (\"Number of connected components: \", nx.number_connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_connected_component(graph):\n",
    "    largest_connected_component = max(nx.connected_component_subgraphs(graph), key=len)\n",
    "    num_nodes = len(largest_connected_component.nodes)\n",
    "    degree = largest_connected_component.degree() \n",
    "    return largest_connected_component, num_nodes, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc, num_nodes, degree = analyze_connected_component(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"largest cc num of nodes: \", num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centrality\n",
    "def centrality(G):\n",
    "    betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "    eigenvec = nx.eigenvector_centrality_numpy(G, weight=\"weight\")\n",
    "    degree = nx.degree_centrality(G)\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    return degree, betweenness, closeness, eigenvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree, betweenness, closeness, eigenvec = centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe for genes and their centrality values\n",
    "def build_dataframe(data, inputs, actual_name):\n",
    "    #inputs is dictionaries within dictionaries\n",
    "    # outer dict: key= centrality type, value: dictionary of computed values for all genes\n",
    "    # inner dict: key= gene id (index or actual name), value: centrality value for the gene id\n",
    "    centrality_df = data[[\"gene_id\"]][:100].copy()\n",
    "    for centrality in inputs.keys():\n",
    "        values = []\n",
    "        for i, gene_id in data[\"gene_id\"][:100].iteritems():\n",
    "            if actual_name:\n",
    "                values.append(inputs[centrality][gene_id])\n",
    "            else:\n",
    "                values.append(inputs[centrality][i])\n",
    "        centrality_df[centrality] = pd.DataFrame(values)\n",
    "    return centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"degree\": degree, \"betweenness\": betweenness, \"closeness\": closeness, \"eigenvector\": eigenvec}\n",
    "centrality_df = build_dataframe(LGG, inputs, False)\n",
    "centrality_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
